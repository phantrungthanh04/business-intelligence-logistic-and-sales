{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","# Đọc dữ liệu file_path = 'cleaned_data.csv'\n","directory_path = '/content/drive/My Drive/[BI Season 8] Workspace/ROUND 2/datasets/'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3aNlxvLeYDg","executionInfo":{"status":"ok","timestamp":1723194466374,"user_tz":-420,"elapsed":20981,"user":{"displayName":"Khanh Tran","userId":"02095708586566525805"}},"outputId":"5f36f538-ca00-4894-ab44-cd6d09944279"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Data preprocessing\n"],"metadata":{"id":"zu6NOm7sWs8N"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import numpy as np\n","import re\n","\n","\n","def join_csv_OM():\n","    dir_path = 'OM\\OM'\n","    files = os.listdir(dir_path)\n","    csv_file = files.pop(0)\n","    df = pd.read_csv(dir_path + '\\\\'+ csv_file, encoding='utf-8')\n","    dtype_spec = (df.dtypes).to_dict()\n","    dtype_spec['from_city'] = 'string'\n","    dtype_spec['to_city'] = 'string'\n","    for k in dtype_spec.keys():\n","        if dtype_spec[k] == 'int64':\n","            dtype_spec[k] = 'float64'\n","    # print(dtype_spec)\n","    df = pd.read_csv(dir_path + '\\\\'+ csv_file, encoding='utf-8', dtype=dtype_spec)\n","    for file in files:\n","        df2 = pd.read_csv(dir_path + '\\\\'+ file, encoding='utf-8', dtype=dtype_spec)\n","        df = pd.concat([df, df2], ignore_index=True)\n","        print(file)\n","    print(df.tail())\n","    df = df.drop_duplicates()\n","    df.to_csv('OM.csv', index=False, encoding='utf-8')\n","\n","def join_csv_LM():\n","    dir_path = 'LM\\LM'\n","    files = os.listdir(dir_path)\n","    csv_file = files.pop(0)\n","    df = pd.read_csv(dir_path + '\\\\'+ csv_file, encoding='utf-8')\n","    # print(dtype_spec)\n","    df = pd.read_csv(dir_path + '\\\\'+ csv_file, encoding='utf-8')\n","    for file in files:\n","        df2 = pd.read_csv(dir_path + '\\\\'+ file, encoding='utf-8')\n","        df = pd.concat([df, df2], ignore_index=True)\n","        print(file)\n","    print(df.tail())\n","    df = df.drop_duplicates()\n","    df['items'] = df['items'].apply(lambda x: remove_next_line(x))\n","    print(df['items'][12948])\n","    # Xóa 1 dòng order_id 238265391\n","    df.to_csv('OM_cleaned.csv', index=False, encoding='utf-8')\n","\n","\n","def remove_next_line(s):\n","    if pd.notna(s):\n","        s = re.sub(r\"\\r\", \" \", s)\n","        return re.sub(r\"\\n\", \" \", s)\n","\n","csv_file_path = 'order_item.csv'\n","df = pd.read_csv(csv_file_path, encoding = 'utf-8')\n","df = df.drop(columns=['Unnamed: 0'])\n","grouped_df = df.groupby(['order_id', 'item_name'], as_index=False).agg({'item_quantity': 'sum'})\n","print(grouped_df)\n"],"metadata":{"id":"dfPERW_nW0xH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tách Order Items"],"metadata":{"id":"gJEvXAchWTcI"}},{"cell_type":"code","source":["import re\n","import json\n","import pandas as pd\n","import ast\n","import numpy as np\n","from tqdm import tqdm\n","\n","\n","def is_nan(value):\n","    if isinstance(value, float):  # Check if the value is a float\n","        return np.isnan(value)  # Use np.isnan only for float types\n","    elif value is None:  # Check for None\n","        return True\n","    else:\n","        return False\n","\n","csv_file_path = 'OM_cleaned.csv'\n","df = pd.read_csv(csv_file_path, encoding = 'utf-8')\n","\n","def extract_pattern(text, pattern):\n","    if text is None:\n","        return None\n","    pattern = re.compile(pattern)\n","    match = pattern.search(text)\n","    if match is None:\n","        return None\n","    else:\n","        return match.group(0)\n","\n","order_id = []\n","item_id = []\n","item_name = []\n","item_quantity = []\n","item_price = []\n","is_dangerous = []\n","k = []\n","price_pattern = r'[0-9]+(\\.[0-9]+)?VND'\n","quantity_pattern = r'\\) x[0-9]+ \\('\n","item_id_pattern = r'[0-9A-Z]+_?VNAMZ-[0-9]+'\n","name_excluded_pattern = r'\\([0-9A-Z]+_?VNAMZ-[0-9]+\\) x[0-9]+ \\([0-9]+(\\.[0-9]+)?VND\\)?'\n","for  indexa, row in tqdm(df.iterrows()):\n","    # print(row)\n","    item_order_id = row['order_id']\n","    item_line = row['items']\n","    if is_nan(item_line): continue\n","    elif re.match(r'\\[{\"item_name\"', str(item_line)):\n","        try:\n","            items_list = ast.literal_eval(str(item_line))\n","            for i in items_list:\n","                order_id.append(item_order_id)\n","                item_id.append(None)\n","                item_price.append(None)\n","                is_dangerous.append(None)\n","                if 'item_name' in i.keys():\n","                    item_name.append((i['item_name']).strip())\n","                else:\n","                    item_name.append(None)\n","                if 'item_quantity' in i.keys():\n","                    item_quantity.append(int(i['item_quantity']))\n","                else:\n","                    item_quantity.append(None)\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            print(f'Order_id: {item_order_id} Line: {item_line}')\n","    elif re.match(r'\\[{\"item_description\"', str(item_line)):\n","        try:\n","            corrected_item_line = re.sub(r'\\bfalse\\b', 'False', str(item_line))\n","            corrected_item_line = re.sub(r'\\btrue\\b', 'True', corrected_item_line)\n","            corrected_item_line = re.sub(r',null', '', corrected_item_line)\n","            items_list = ast.literal_eval(corrected_item_line)\n","            for i in items_list:\n","                order_id.append(item_order_id)\n","                item_id.append(None)\n","                item_price.append(None)\n","                if 'item_description' in i.keys():\n","                    item_name.append(i['item_description'].strip())\n","                else:\n","                    item_name.append(None)\n","                if 'quantity' in i.keys():\n","                    item_quantity.append(int(i['quantity']))\n","                else:\n","                    item_quantity.append(None)\n","                if 'is_dangerous_good' in i.keys():\n","                    is_dangerous.append(i['is_dangerous_good'])\n","                else:\n","                    is_dangerous.append(None)\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            print(f'Order_id: {item_order_id} Line: {item_line}')\n","    else:\n","        items_list = str(item_line).split('), ')\n","        for i in items_list:\n","            price = extract_pattern(extract_pattern(i, price_pattern), r'[0-9]+')\n","            quantity = extract_pattern(extract_pattern(i, quantity_pattern), r'[0-9]+')\n","            id = extract_pattern(i, item_id_pattern)\n","            name = (re.sub(name_excluded_pattern, \"\", i)).strip()\n","            if price is None:\n","                item_price.append(None)\n","            else:\n","                item_price.append(int(price))\n","            if quantity is None:\n","                item_quantity.append(None)\n","            else:\n","                item_quantity.append(int(quantity))\n","            if id is None:\n","                item_id.append(None)\n","            else:\n","                item_id.append(id)\n","            if name is None:\n","                item_name.append(None)\n","            else:\n","                item_name.append(name.strip())\n","            order_id.append(item_order_id)\n","            is_dangerous.append(None)\n","\n","data = {'order_id': order_id, 'item_name': item_name, 'item_id': item_id, 'item_quantity': item_quantity, 'item_price': item_price, 'is_dangerous_good': is_dangerous}\n","df = pd.DataFrame(data)\n","df.to_csv('order_item.csv', encoding='utf-8')\n"],"metadata":{"id":"CQTMGzVNWis1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Order Status Analysis"],"metadata":{"id":"vDB8qkHKWLkB"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"kgTCIjwMa_cI","executionInfo":{"status":"error","timestamp":1723252691023,"user_tz":-420,"elapsed":1597,"user":{"displayName":"Khanh Tran","userId":"02095708586566525805"}},"outputId":"92219a83-aef5-4d17-fc1c-2e409963e1e5"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'directory_path' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-780433e2ed7b>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0morder_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickup_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeliver_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrts_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplete_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Load and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mcal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'ryo_calendar (1).csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mcal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0morder_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'OM_cleaned.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'directory_path' is not defined"]}],"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","\n","status_col = ['created', 'pickup', 'deliver', 'rts', 'complete']\n","\n","def check1(d_start, d_end, rts):\n","    if pd.isna(rts):\n","        return False\n","    else:\n","        return (rts <= d_end) & (d_start <= rts)\n","\n","def get_counts(order_df, d_start):\n","  d_end = d_start + pd.DateOffset(days=1) - pd.DateOffset(seconds=1)\n","  for c in status_col:\n","    order_df[c] = 0\n","  order_df.loc[((d_start <= order_df['creation_datetime']) & (order_df['creation_datetime'] <= d_end)) | ((d_start > order_df['creation_datetime']) & (d_end < order_df['pickup_datetime'])), 'created'] = 1\n","  order_df.loc[((d_start <= order_df['pickup_datetime']) & (order_df['pickup_datetime'] <= d_end)) | ((d_start > order_df['pickup_datetime']) & (d_end < order_df['first_valid_delivery_attempt_datetime'])), 'pickup'] = 1\n","  order_df.loc[((d_start <= order_df['first_valid_delivery_attempt_datetime']) & (order_df['first_valid_delivery_attempt_datetime'] <= d_end)) | ((d_start > order_df['first_valid_delivery_attempt_datetime']) & (d_end < order_df['rts_trigger_datetime'])), 'deliver'] = 1\n","  order_df.loc[((d_start <= order_df['rts_trigger_datetime']) & (order_df['rts_trigger_datetime'] <= d_end)) | ((d_start > order_df['rts_trigger_datetime']) & (d_end < order_df['delivery_success_datetime'])), 'rts'] = 1\n","  order_df.loc[((d_start <= order_df['delivery_success_datetime']) & (order_df['delivery_success_datetime'] <= d_end)), 'complete'] = 1\n","  create_count = (order_df['created'] == 1).sum()\n","  pickup_count = (order_df['pickup'] == 1).sum()\n","  deliver_count = (order_df['deliver'] == 1).sum()\n","  rts_count = (order_df['rts'] == 1).sum()\n","  complete_count = (order_df['complete'] == 1).sum()\n","  order_df = order_df.drop(order_df[order_df['complete'] == 1].index)\n","  return order_df, create_count, pickup_count, deliver_count, rts_count, complete_count\n","\n","def get_counts2(order_df, d_start):\n","  d_end = d_start + pd.DateOffset(days=1) - pd.DateOffset(seconds=1)\n","  for c in status_col:\n","    order_df[c] = 0\n","  order_df.loc[((d_start <= order_df['creation_datetime']) & (order_df['creation_datetime'] <= d_end)) | ((d_start > order_df['creation_datetime']) & (d_end < order_df['pickup_datetime'])), 'created'] = 1\n","  order_df.loc[((d_start <= order_df['pickup_datetime']) & (order_df['pickup_datetime'] <= d_end)) | ((d_start > order_df['pickup_datetime']) & (d_end < order_df['first_valid_delivery_attempt_datetime'])), 'pickup'] = 1\n","  order_df.loc[((d_start <= order_df['first_valid_delivery_attempt_datetime']) & (order_df['first_valid_delivery_attempt_datetime'] <= d_end)) | ((d_start > order_df['first_valid_delivery_attempt_datetime']) & (d_end < order_df['rts_trigger_datetime'])), 'deliver'] = 1\n","  order_df.loc[((d_start <= order_df['rts_trigger_datetime']) & (order_df['rts_trigger_datetime'] <= d_end)) | ((d_start > order_df['rts_trigger_datetime']) & (d_end < order_df['delivery_success_datetime'])), 'rts'] = 1\n","  order_df.loc[((d_start <= order_df['delivery_success_datetime']) & (order_df['delivery_success_datetime'] <= d_end)), 'complete'] = 1\n","  create_count = (order_df['created'] == 1).sum()\n","  pickup_count = (order_df['pickup'] == 1).sum()\n","  deliver_count = (order_df['deliver'] == 1).sum()\n","  rts_count = (order_df['rts'] == 1).sum()\n","  complete_count = (order_df['complete'] == 1).sum()\n","  order_df = order_df.drop(order_df[order_df['complete'] == 1].index)\n","  return order_df, create_count, pickup_count, deliver_count, rts_count, complete_count\n","# Load and prepare data\n","cal_df = pd.read_csv(directory_path+'ryo_calendar (1).csv', encoding='utf-8')\n","cal_df['date'] = pd.to_datetime(cal_df['date'])\n","order_df = pd.read_csv(directory_path+'OM_cleaned.csv', encoding='utf-8')\n","\n","date_col = ['creation_datetime', 'pickup_datetime', 'first_valid_delivery_attempt_datetime', 'rts_trigger_datetime', 'delivery_success_datetime']\n","for c in date_col:\n","    order_df[c] = pd.to_datetime(order_df[c], format='mixed')\n","for c in status_col:\n","    order_df[c] = None\n","# Lists to store results\n","created = []\n","picked_up = []\n","delivered = []\n","rts_triggered = []\n","completed = []\n","\n","# Calculate counts\n","for i, row in tqdm(cal_df.iterrows()):\n","    d_start = row['date']\n","    order_df, created_count, pickup_count , deliver_count, rts_triggered_count, completed_count = get_counts(order_df, d_start)\n","    print(f'{d_start}: {created_count}, {pickup_count}, {deliver_count}, {rts_triggered_count}, {completed_count}')\n","    created.append(created_count)\n","    picked_up.append(pickup_count)\n","    delivered.append(deliver_count)\n","    rts_triggered.append(rts_triggered_count)\n","    completed.append(completed_count)\n","\n","# Add results to the calendar DataFrame\n","cal_df['created'] = created\n","cal_df['picked_up'] = picked_up\n","cal_df['delivered'] = delivered\n","cal_df['rts_triggered'] = rts_triggered\n","cal_df['completed'] = completed\n","\n","# Print the final DataFrame\n","print(cal_df)\n","cal_df.to_csv('new_cal.csv', index=False)\n"]}]}